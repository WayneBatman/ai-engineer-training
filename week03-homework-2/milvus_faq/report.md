# Milvus FAQ 检索系统 - 实验结果分析报告

## 1. 项目简介

本项目成功构建了一个基于 LlamaIndex 和 Milvus 的 FAQ 问答系统。系统通过 FastAPI 提供了 RESTful API 接口，能够接收用户的自然语言问题，并从知识库中检索最相关的 FAQ 条目。

核心技术栈包括：

-   **API 框架**: FastAPI
-   **索引与检索**: LlamaIndex
-   **向量存储**: Milvus(Docker版本)
-   **嵌入模型**: 通义千问 `text-embedding-v3`
-   **文档处理**: LlamaIndex 内置的语义切分器 (`SemanticSplitterNodeParser`)

项目实现了如下功能
- 通过fastAPI，对外提供检索和热更新接口，为后续前端调试提供便利
- 利用Milvus向量数据库+LlamaIndex实现用户自然语言的问答检索

## 2. 核心功能测试

### 2.1. 查询准确性测试

结果如下

| 测试问题 | 预期结果                   | 实际返回 (Top 2) 及得分（Score）                       | 结果分析                                                              |
| :--- |:-----------------------|:----------------------------------------------|:------------------------------------------------------------------|
| "管家几点上班？" | 匹配 "管家的上班时间？"| "管家的上班时间？" - 0.836/"楼下好吵啊？" - 0.480           | **成功**。用户的口语化提问与标准问题在语义上高度相关，`text-embedding-v3` 模型准确捕捉到了这种相似性。   |
| "附近有没有什么超市？" | 语义匹配 "附近有没有什么超市？"         | "附近有没有什么超市？"  - 0.811 /我这个月的物业费为啥这么高？ - 0.421 | **成功**。精准匹配到附近超市的问题，但是可能是因为语料库中问题都暗含联系管家的意思，导致第二个查询到我这个月的物业费为啥这么高 |
| "极客是谁？" | 无相关答案                  | "楼下好吵啊？" -0.333/"我这个月的物业费为啥这么高？" -0.290            | **成功**。知识库中没有关于极客的信息，返回的条目得分很低，应用层可以根据阈值（如 0.7）轻松过滤掉这些不相关的结果。     |

**结论**: 当前程序表现比较符合预期，表现良好。既能精准匹配，同时对用户的自然语言的处理，也是比较优秀

### 2.2. 热更新功能测试

热更新功能通过 `/api/update-index` 接口进行测试。

1.  **初始状态**: 启动服务，查询 "怎么交物业费？"，系统返回关于 "我这个月的物业费为啥这么高？"/ "楼下好吵啊？"的答案。

    不符合用户的预期回复
2.  **修改数据**: 在 `data/faq.csv` 文件中添加一行新数据：
    ```csv
    question,answer
    ...
    "物业费的缴纳方式？","业主您好，物业费缴纳可以通过业主端的我的账单、联系管家查询物业费之后分享缴费小程序、移步物业前台现场等3种方式进行缴纳，支持支付宝、微信、现金、转账等支付方式"
    ```
3.  **触发更新**: 调用 `POST /api/update-index` 接口。观察服务器日志，显示索引开始并成功完成更新。
4.  **验证更新**: 再次查询 "怎么交物业费？"。
    -   **结果**: 系统成功返回了新添加的关于 "物业费缴纳方式" 的问答条目，且得分很高（0.8064）。

**结论**: 热更新功能运行符合预期。系统能够动态地重新加载和索引知识库文件，无需重启服务即可让变更生效，极大地提高了系统的可维护性。

## 3. 关键技术点分析

### 3.1. 语义切分 (`SemanticSplitterNodeParser`)

-   **优势**: 相比于传统的固定长度切分，语义切分器能根据句子间的语义关系来决定断点。对于 FAQ 这种 "问题-答案" 成对出现的短文本，它能确保每个 `Document` 块（Node）包含一个完整的问答对，避免了将一个完整的问答对错误地切分到两个独立的块中。
-   **分析**: 这种优化保证了每个向量都代表了一个完整且独立的语义单元（一个FAQ），从而提高了检索的精准度。当答案文本较长时，此优势会更加明显。

## 4. 总结与展望

本次实验成功构建了一个功能完善、易于维护的 FAQ 检索系统。实验结果表明，基于 LlamaIndex 和 Milvus 的架构，结合高质量的嵌入模型，能够高效、准确地解决实际场景中的问答需求。

**遇到的问题**:
1. 开发环境为windows，但是Milvus Lite只能支持Linux/Macos。这导致只能通过部署Docker版本的Milvus。然而，当前版本的llama_index.vector_stores.milvus默认是异步调用，这导致部分代码需要增加async/await方式进行调用。
2. 目前即使用户准确输入faq.csv中的问题，其得分也只有0.8左右，并未达到0.9及以上。这个可能和选择的嵌入的TEXT_EMBEDDING_V3模型有关，看看微调之后，会不会好点。

**未来可扩展方向**:
1.  **答案生成**: 对于没有直接匹配答案的问题，可以引入 LLM（如通义千问 `qwen-plus`），将检索到的最相关的几个 FAQ 作为上下文，生成一个更具概括性的、定制化的答案。
2.  **安全问题**: 根据实际生产环境的要求，系统的核心组件fastapi/Milvus等，需要配置用户名/密码来保证数据安全性，满足生产的等保要求。
